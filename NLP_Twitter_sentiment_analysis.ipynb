{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP-Twitter-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shailendrarg/NLP/blob/master/NLP_Twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "52bf27bebf8e95c02bb8b123a1d535075e8376c4",
        "id": "2atCpFwqkNNv",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ff9e92acf175c4e823c9536f6c5cc4268a0e8115",
        "id": "nuG8_TK4kNNx",
        "colab_type": "text"
      },
      "source": [
        "This kernel is the solution which consist of building a system that can classify tweets as Sad or Happy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "85b0c81cca1fe16ef1b65c470e5aa989656c8fa2",
        "id": "ia0M7Tl1kNNz",
        "colab_type": "text"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "61b9efbf1cce15100bb9526da2017193b001fac5",
        "id": "k1fG293HkNN1",
        "colab_type": "text"
      },
      "source": [
        "We will start by reading some tweets so we can understand our data better. We will then try to transform our tweets into something usable by different ML models, where we are going to choose the more efficient. We will finally fine tune our model and then test it to see its efficiency on new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6a11a0e30ff6e91ff073074521706bbcff59c47e",
        "id": "Rja78fN3kNN3",
        "colab_type": "text"
      },
      "source": [
        "Observations:\n",
        "\n",
        "1.I used lemmatization instead of steaming\n",
        "\n",
        "2.I also noticed that I was mistaken when I stopped the max_features parameter at 20000 while doing GridSearch, I should have tested a bigger one, because if it stopped at 20000 (which is the max), it may get better using a bigger one. I just added None (no limit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1a6afc9118eca64bc4bb552cced34b42ca9c73f8",
        "id": "27idYxbFkNN5",
        "colab_type": "text"
      },
      "source": [
        "### Content\n",
        "\n",
        "- [Loading the data](#Load-the-data)\n",
        "- [Visualize the tweets](#Visualize-the-tweets)\n",
        "    - [Emoticons](#Emoticons)\n",
        "    - [Most used words](#Most-used-words)\n",
        "    - [Stop words](#Stop-words)\n",
        "    - [Stemming](#Stemming)\n",
        "- [Prepare the data](#Prepare-the-data)\n",
        "    - [Bag of Words](#Bag-of-Words)\n",
        "    - [Building the pipeline](#Building-the-pipeline)\n",
        "- [Select a model](#Select-a-model)\n",
        "- [Fine tune the model](#Fine-tune-the-model)\n",
        "- [Testing the model](#Test)\n",
        "    - [Test your tweet](#Test-your-tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "414a6b7e831613c3c9a88d9633fa412de72ebd8a",
        "id": "DLV4sDFZkNN7",
        "colab_type": "text"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "5uIBS6K4kNN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# This is for making some large tweets to be displayed\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "# I got some encoding issue, I didn't knew which one to use !\n",
        "# This post suggested an encoding that worked!\n",
        "# https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\n",
        "#train_data = pd.read_csv(\"C:\\\\Users\\\\vidya\\\\Downloads\\\\Twitter sentiment analysis\\\\Data\\\\train.csv\", encoding='ISO-8859-1')\n",
        "train_data = pd.read_csv(\"/content/drive/My Drive/Machine Learning/Twitter Sentiment analysis/twitter-sentiment-analysis2/train.csv\", encoding='ISO-8859-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2df12f6473bd02427771d3ca0ae2de291ec03b83",
        "id": "j73bxIYqkNOA",
        "colab_type": "code",
        "outputId": "cef0c571-863e-44ee-d275-6f4b88ccc321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "train_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL friend.............</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trailer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99984</th>\n",
              "      <td>99996</td>\n",
              "      <td>0</td>\n",
              "      <td>@Cupcake  seems like a repeating problem   hope you're able to find something.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99985</th>\n",
              "      <td>99997</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99986</th>\n",
              "      <td>99998</td>\n",
              "      <td>0</td>\n",
              "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99987</th>\n",
              "      <td>99999</td>\n",
              "      <td>1</td>\n",
              "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99988</th>\n",
              "      <td>100000</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake_kayla haha yes you do</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99989 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ItemID  ...                                                                                        SentimentText\n",
              "0           1  ...                                                             is so sad for my APL friend.............\n",
              "1           2  ...                                                                     I missed the New Moon trailer...\n",
              "2           3  ...                                                                              omg its already 7:30 :O\n",
              "3           4  ...            .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...\n",
              "4           5  ...                                                         i think mi bf is cheating on me!!!       T_T\n",
              "...       ...  ...                                                                                                  ...\n",
              "99984   99996  ...                       @Cupcake  seems like a repeating problem   hope you're able to find something.\n",
              "99985   99997  ...  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...\n",
              "99986   99998  ...                                                                       @CuPcAkE_2120 ya i thought so \n",
              "99987   99999  ...                                        @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me. \n",
              "99988  100000  ...                                                                      @cupcake_kayla haha yes you do \n",
              "\n",
              "[99989 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "20b16d52f53f8b3005e5728ee36b3e3b8f5e6461",
        "id": "fEaQrtwzkNOD",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "21098fa5bbbbcdb299b45ba1f7ab46013b4656f2",
        "id": "8ZiUA5nXkNOE",
        "colab_type": "text"
      },
      "source": [
        "From the tweets above, we can already make some remarks about the data:\n",
        "\n",
        "- We can see that there is some garbage like '&amp', '&lt' (which are basically used in HTML) that aren't gonna help us in our classification\n",
        "- In twitter, people mention their friends with tags like @username, there is a lot of them in our data. I was discussing with a friend about the usefulness of tags in our classification, for him, people tend to mention more friends when they are happy, but I think that people may mention people because they made bad things. When we face this kind of uncertainty, it's better to try the different options and evaluate which will do well, this is what we are gonna do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c2dfe4d1b2a68571cf47b270e61d7ec01458efa6",
        "id": "eDnNBsnkkNOF",
        "colab_type": "code",
        "outputId": "9ab47698-2619-41dd-8da3-3db539f8a312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# We will now take a look at random tweets\n",
        "# to gain more insights\n",
        "\n",
        "rand_indexs = np.random.randint(1,len(train_data),50).tolist()\n",
        "train_data[\"SentimentText\"][rand_indexs]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4439                                                               please!! hope and time.. nothing more! D:\n",
              "32259                                                          @againtoday i'm hear ya dude, i can't either \n",
              "34213                                                          @AJlovesmusic yes i know. MIMI: STOP PLEASE, \n",
              "74795                                                          @bryanchauvel Wooo - hoo!  That is exciting!!\n",
              "85025                        @cbryant68 it will be fun. PS I'm not coming to work today. I don't feel good. \n",
              "66689                                                     @britgeekgrrl every day is a bad day in my office \n",
              "81789                                                         @carolinekristek its a mutual awesomeness lol \n",
              "68822    @BStyleINC Nope will do WED O didn't tell the mac died June 18, 2009...Let's Us pray  RIP Oct. 2...\n",
              "59971                                                                      @bforcefield NOOOO! It cant be!! \n",
              "77950                 @cazp09 heyy  im good  thanx u    my  friend  which    had  it for nearly a  year  xxx\n",
              "94238                         @corrinelynn Heyy corbor  would you mind asking Uncle Brian a question for me?\n",
              "95091                                                        @ciaweth soda? Unsure - just showed up @ work. \n",
              "73188    @brittneeclark It used to be Blue. But I think it has recently changed to red. But his offical f...\n",
              "47455                              @ArcticSensation ... Lmao XD no thanks, a lot of the guys here are gross \n",
              "65589    @brentitude our boy wake either has it or not. Looked like he had it but lost it there  #redsox ...\n",
              "77506                                                           @cathybaron I was afraid of clowns as a kid \n",
              "95780                                                         @crbear psyched to finally meet you tomorrow! \n",
              "7037             #sad. Taking our cat to be put to sleep tomorrow. Last night tonight  http://mypict.me/4mMp\n",
              "26887                                                                              @A7X_Bat_Angel try again \n",
              "5639       naiinis na qo. x( lahat pa nman ng duda qo TOTOO. buset na ean!! (angry) http://plurk.com/p/z3v9d\n",
              "59880                                               @bellanicola Wait until exactly midnight to get it LOL! \n",
              "57946                    @beaky22 i'm so glad i was able to make your day because you make mine so often!!! \n",
              "4088                                                                      my dog i wanted got adopted today,\n",
              "76069    @BunnyBridget haha! a hot dog with YELLOW mustard...did you watch kendra? i was excited to see y...\n",
              "10907         #marsiscoming #marsiscoming #marsiscoming #marsiscoming howeva. why Cologne? why not Berlin?! \n",
              "32623    @AGingerSnaps - Jared! You aren't following me! I consider this a problem! What the heck?  Hope ...\n",
              "70387                                                     @bush72 thanks for trying to check my id tonight. \n",
              "22370                                                                  @_xotashhh Now I'm dying for moreee. \n",
              "67895    @brugger Hi, Brian. Bronner's has about 60 billboards, primarily in Michigan &amp; the Midwest. ...\n",
              "66687                                                                 @britesprite What are your interests? \n",
              "11564              (cont.)And is going to bed after this LifeTime movie. So ima say Goodnightttt now  &lt;33\n",
              "93961          @bdeffenbacher try and check out Chick-fil-a or Boston Market. We don't have those in Oregon \n",
              "32122                                     @alfecia did that last week....no cavities for both!!!! Thankful! \n",
              "25305                                               @30secondstomars http://twitpic.com/6q0d4 - hiii Steve! \n",
              "1457                                                      I hat3 seeing friends breakup.....im sorry frankus\n",
              "92945    @Bass_ @koist You two are far too naughty!  But I it's all good fun   Changed it to @SueB_  I kn...\n",
              "6687     #musicmonday I &lt;3 my new song... I AM ANTHM!!  I'm gonna nap for lunch enjoy your day!!  good...\n",
              "61022    @bettermzansi Our S.A. Team also needs a far better &quot;dress code&quot;.  Comments in oversea...\n",
              "26470    @A_Montenegro Thanks, Angela! It turned out great. Hours of calculations and all I had to do was...\n",
              "76653                                                                @ButterflyerGurl Meh I hate all of you \n",
              "96293    @Classy_Traceye I gt an old soul wht can I say haaaaa! But she had her club face on n the gym ey...\n",
              "41463                                                           @andybeal guess I won't be enabling that so \n",
              "89292              @Claire_Cordon  so is there another season of House comin up or was this the last season?\n",
              "12041    &amp; I'd like to thank @anthonycastro3 &amp; @whateverr05 for being really good genuine friends...\n",
              "54737    @Aymsters and I also have a pale skin! But I can go to the beach when I'm bored  and even though...\n",
              "22785                                                       @1047FishTraffic WE LOVE OUR TRAFFIC PRINCESS!! \n",
              "62377                  @BlackEyeDesign Sure  But of you want beer you'll have to bring it. Miss you already.\n",
              "92014    @banshe1999 yeah...jill lives a couple of towns away....like a 15 minute drive....but i'm actual...\n",
              "75228    @carlottap  It was instantaneous to mine! Maybe refresh a few times? It drove me nots when it wo...\n",
              "40859           @amskiwankanobe Then you have one kick ass desktop!  I think that picture's sooooo adorable \n",
              "Name: SentimentText, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "28f30b8fef70f32335fafa9dc847cec11321b759",
        "id": "N5K-qV17kNOI",
        "colab_type": "text"
      },
      "source": [
        "#### Note\n",
        "You will not have the same results at each execution because of the randomization. For me, after some execution, I noticed this:\n",
        "- There is tweets with a url (like tweet 35546): we must think about a way to handle URLs, I thought about deleting them because a domain name or the protocol used will not make someone happy or sad unless the domain name is 'food.com'.\n",
        "- The use of hashtags: we should keep only the words without '#' so words like python and the hashtag '#python' can be seen as the same word, and of course they are.\n",
        "- Words like 'as', 'to' and 'so' should be deleted, because they only serve as a way to link phrases and words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e1ae5c3aa81d998b37d87fa926d9682cd71358f2",
        "id": "VC0QUHHskNOJ",
        "colab_type": "text"
      },
      "source": [
        "#### Emoticons\n",
        "The internet language includes so many emoticons, people also tend to create their own, so we will first analyze the emoticons included in our dataset, try to classify them as happy and said, and make sure that our model know about them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "406019e45c7d1c06f1739be273534a42e8002f42",
        "id": "Ze9vxLQXkNOK",
        "colab_type": "code",
        "outputId": "33e7edba-273d-4106-c93d-b56797ee2d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We are gonna find what emoticons are used in our dataset\n",
        "import re\n",
        "tweets_text = train_data.SentimentText.str.cat()\n",
        "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
        "emos_count = []\n",
        "for emo in emos:\n",
        "    emos_count.append((tweets_text.count(emo), emo))\n",
        "sorted(emos_count,reverse=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3281, ':/'),\n",
              " (2874, 'x '),\n",
              " (2626, ': '),\n",
              " (1339, 'x@'),\n",
              " (1214, 'xx'),\n",
              " (1162, 'xa'),\n",
              " (984, ';3'),\n",
              " (887, 'xp'),\n",
              " (842, 'xo'),\n",
              " (713, ';)'),\n",
              " (483, 'xe'),\n",
              " (431, ';I'),\n",
              " (353, ';.'),\n",
              " (254, 'xD'),\n",
              " (251, 'x.'),\n",
              " (245, '::'),\n",
              " (234, 'X '),\n",
              " (217, ';t'),\n",
              " (209, ';s'),\n",
              " (185, ':O'),\n",
              " (176, ':3'),\n",
              " (166, ';D'),\n",
              " (159, \":'\"),\n",
              " (157, 'XD'),\n",
              " (146, 'x3'),\n",
              " (142, ':p'),\n",
              " (126, \":'(\"),\n",
              " (118, ':@'),\n",
              " (117, 'xh'),\n",
              " (117, ':S'),\n",
              " (109, 'xm'),\n",
              " (104, ';p'),\n",
              " (104, ';-)'),\n",
              " (92, ':|'),\n",
              " (91, 'x,'),\n",
              " (89, ';P'),\n",
              " (76, 'xd'),\n",
              " (75, ';o'),\n",
              " (75, ';d'),\n",
              " (71, ':o'),\n",
              " (65, 'XX'),\n",
              " (63, ':L'),\n",
              " (59, 'Xx'),\n",
              " (59, ':1'),\n",
              " (58, ':]'),\n",
              " (57, ':s'),\n",
              " (56, ':0'),\n",
              " (54, 'XO'),\n",
              " (44, ';;'),\n",
              " (43, ';('),\n",
              " (38, ':-D'),\n",
              " (37, 'xk'),\n",
              " (36, 'XT'),\n",
              " (35, 'x?'),\n",
              " (35, 'x)'),\n",
              " (34, 'x2'),\n",
              " (33, ';/'),\n",
              " (32, 'x:'),\n",
              " (32, ':\\\\'),\n",
              " (31, 'x-'),\n",
              " (27, 'Xo'),\n",
              " (27, 'XP'),\n",
              " (27, ':-/'),\n",
              " (26, ':-P'),\n",
              " (25, ':*'),\n",
              " (23, 'xX'),\n",
              " (22, \":')\"),\n",
              " (17, 'xP'),\n",
              " (16, ':['),\n",
              " (16, ':-p'),\n",
              " (14, 'x]'),\n",
              " (14, 'XM'),\n",
              " (13, ':-O'),\n",
              " (12, 'x('),\n",
              " (12, 'X1'),\n",
              " (12, ':x'),\n",
              " (11, 'XS'),\n",
              " (11, ':l'),\n",
              " (10, 'x*'),\n",
              " (10, 'X.'),\n",
              " (10, ':b'),\n",
              " (10, ':T'),\n",
              " (9, ';]'),\n",
              " (9, ':I'),\n",
              " (8, ':C'),\n",
              " (7, ';-('),\n",
              " (7, ':-|'),\n",
              " (6, 'X,'),\n",
              " (6, ':-o'),\n",
              " (6, ':-\\\\'),\n",
              " (6, ':-*'),\n",
              " (6, ':$'),\n",
              " (5, 'XL'),\n",
              " (5, ':d'),\n",
              " (5, ':X'),\n",
              " (5, ':H'),\n",
              " (5, ':?'),\n",
              " (5, ':-S'),\n",
              " (4, ';-D'),\n",
              " (3, ':Z'),\n",
              " (3, ':E'),\n",
              " (3, ':-s'),\n",
              " (3, ':-['),\n",
              " (3, ':-X'),\n",
              " (2, 'X5'),\n",
              " (2, 'X-('),\n",
              " (2, \"X's\"),\n",
              " (2, ';-;'),\n",
              " (2, ':}'),\n",
              " (2, ':D'),\n",
              " (2, ':;'),\n",
              " (2, \":'D\"),\n",
              " (1, 'x|'),\n",
              " (1, \"x'd\"),\n",
              " (1, \"x'D\"),\n",
              " (1, ';-|'),\n",
              " (1, ';-/'),\n",
              " (1, ':-x'),\n",
              " (1, ':-h'),\n",
              " (1, ':-]'),\n",
              " (1, ':-W'),\n",
              " (1, ':-$'),\n",
              " (1, ':('),\n",
              " (1, \":'[\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d8c5735902af0c0e4f485fde08cbd4af267bb37d",
        "id": "6ASpMkehkNOM",
        "colab_type": "text"
      },
      "source": [
        "We should by now know which emoticons are used (and its frequency) to build two regex, one for the happy ones and another for the sad ones. We will then use them in the preprocessing process to mark them as using happy emoticons or sad ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fd47063d8a7d3be29b1e24f39ca4e4ce8411f815",
        "id": "a9gHaXVFkNOQ",
        "colab_type": "code",
        "outputId": "a9f3581d-9555-44c7-e2fb-3e5f72606a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
        "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
        "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
        "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Happy emoticons: {';P', 'xd', ';D', ';)', ';-)', 'x)', ';d', ';p', ':d', ':-D', 'xD', ':D', ';-D', 'XD', ':p'}\n",
            "Sad emoticons: {':/', ':|', ':(', \":'(\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a0b0964a5bab89f2f9e235bb5a6bf384fe86c3c6",
        "id": "fKJYrsYWkNOU",
        "colab_type": "text"
      },
      "source": [
        "#### Most used words\n",
        "What we are going to do next is to define a function that will show us top words, so we may fix things before running our learning algorithm. This function takes as input a text and output words sorted according to their frequency, starting with the most used word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d3eea0c5a024a8844060fea507a5ff374d765254",
        "id": "dvXS8RwRkNOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1c97ba4-2275-4d08-89d2-3b174174680e"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Uncomment this line if you haven't downloaded punkt before\n",
        "# or just run it as it is and uncomment it if you got an error.\n",
        "#nltk.download('punkt')\n",
        "def most_used_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    frequency_dist = nltk.FreqDist(tokens)\n",
        "    print(\"There is %d different words\" % len(set(tokens)))\n",
        "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "44c3a1b1bf2b665c537534bddebc1b78714a8f73",
        "scrolled": false,
        "id": "-tlPfhq6kNOY",
        "colab_type": "code",
        "outputId": "3b7b11a1-ed80-4cba-cd9e-3c61b7117077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "most_used_words(train_data.SentimentText.str.cat())[:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is 134028 different words\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@',\n",
              " '!',\n",
              " '.',\n",
              " 'I',\n",
              " ',',\n",
              " 'to',\n",
              " 'the',\n",
              " 'you',\n",
              " '?',\n",
              " 'a',\n",
              " 'it',\n",
              " 'i',\n",
              " '...',\n",
              " ';',\n",
              " 'and',\n",
              " '&',\n",
              " 'my',\n",
              " 'for',\n",
              " 'is',\n",
              " 'that',\n",
              " \"'s\",\n",
              " \"n't\",\n",
              " 'in',\n",
              " 'of',\n",
              " 'me',\n",
              " 'have',\n",
              " 'on',\n",
              " 'quot',\n",
              " \"'m\",\n",
              " 'so',\n",
              " ':',\n",
              " 'but',\n",
              " '#',\n",
              " 'do',\n",
              " 'was',\n",
              " 'be',\n",
              " 'not',\n",
              " 'your',\n",
              " 'are',\n",
              " 'just',\n",
              " 'with',\n",
              " 'like',\n",
              " '-',\n",
              " 'at',\n",
              " 'too',\n",
              " 'get',\n",
              " 'good',\n",
              " 'u',\n",
              " 'up',\n",
              " 'know',\n",
              " 'all',\n",
              " 'this',\n",
              " 'now',\n",
              " 'no',\n",
              " 'we',\n",
              " 'out',\n",
              " ')',\n",
              " 'love',\n",
              " 'can',\n",
              " '(',\n",
              " 'what',\n",
              " 'one',\n",
              " 'will',\n",
              " 'lol',\n",
              " 'go',\n",
              " 'about',\n",
              " 'did',\n",
              " \"'ll\",\n",
              " 'got',\n",
              " 'amp',\n",
              " 'there',\n",
              " 'day',\n",
              " 'http',\n",
              " 'see',\n",
              " \"'re\",\n",
              " 'if',\n",
              " 'time',\n",
              " 'they',\n",
              " 'think',\n",
              " 'as',\n",
              " 'when',\n",
              " 'from',\n",
              " 'You',\n",
              " 'It',\n",
              " 'going',\n",
              " 'really',\n",
              " 'am',\n",
              " 'work',\n",
              " 'well',\n",
              " 'had',\n",
              " 'would',\n",
              " 'how',\n",
              " 'he',\n",
              " 'here',\n",
              " 'some',\n",
              " 'thanks',\n",
              " 'back',\n",
              " 'im',\n",
              " 'haha',\n",
              " 'or']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "27922113ab86ed096339614ee6616dfd6cf259fb",
        "id": "HoUzBLLdkNOd",
        "colab_type": "text"
      },
      "source": [
        "#### Stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3c24b5a72ed93b029c8492aee5e527dc81e898c7",
        "id": "F4BgDuO3kNOf",
        "colab_type": "text"
      },
      "source": [
        "What we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f4RC7-amyd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1456a9da-0545-4147-ea44-0a50e6ddbde9"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "760928a6057f2b8480d3c4f10ed3b2709ff62c6e",
        "id": "YsN0SZ7YkNOg",
        "colab_type": "code",
        "outputId": "3397dfbc-c7e6-4b8d-aee3-c60e95c2b8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "#nltk.download(\"stopwords\")\n",
        "\n",
        "mw = most_used_words(train_data.SentimentText.str.cat())\n",
        "most_words = []\n",
        "for w in mw:\n",
        "    if len(most_words) == 1000:\n",
        "        break\n",
        "    if w in stopwords.words(\"english\"):\n",
        "        continue\n",
        "    else:\n",
        "        most_words.append(w)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is 134028 different words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e8150fd219b8c83541820f373e345baaae29c1e0",
        "id": "_HW62zrmkNOj",
        "colab_type": "code",
        "outputId": "1e999244-dcd3-4e8e-b043-c96f21dccc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# What we did is to filter only non stop words.\n",
        "# We will now get a look to the top 1000 words\n",
        "sorted(most_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " \"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '*hugs*',\n",
              " '*sigh*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '--',\n",
              " '.',\n",
              " '..',\n",
              " '...',\n",
              " '/',\n",
              " '1',\n",
              " '10',\n",
              " '100',\n",
              " '12',\n",
              " '1st',\n",
              " '2',\n",
              " '20',\n",
              " '2nd',\n",
              " '3',\n",
              " '30',\n",
              " '30SECONDSTOMARS',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " ':',\n",
              " ';',\n",
              " '=',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'AND',\n",
              " 'Ah',\n",
              " 'AlexAllTimeLow',\n",
              " 'All',\n",
              " 'Also',\n",
              " 'Alyssa_Milano',\n",
              " 'Am',\n",
              " 'And',\n",
              " 'Are',\n",
              " 'As',\n",
              " 'At',\n",
              " 'Aw',\n",
              " 'Awesome',\n",
              " 'Aww',\n",
              " 'Awww',\n",
              " 'BSB',\n",
              " 'Birthday',\n",
              " 'But',\n",
              " 'Ca',\n",
              " 'Can',\n",
              " 'Chris',\n",
              " 'Come',\n",
              " 'Congrats',\n",
              " 'Cool',\n",
              " 'D',\n",
              " 'DM',\n",
              " 'DO',\n",
              " 'Damn',\n",
              " 'Day',\n",
              " 'Did',\n",
              " 'Do',\n",
              " 'Enjoy',\n",
              " 'FF',\n",
              " 'Follow',\n",
              " 'FollowFriday',\n",
              " 'For',\n",
              " 'Friday',\n",
              " 'Get',\n",
              " 'Glad',\n",
              " 'Go',\n",
              " 'God',\n",
              " 'Good',\n",
              " 'Got',\n",
              " 'Great',\n",
              " 'Had',\n",
              " 'Haha',\n",
              " 'Happy',\n",
              " 'Have',\n",
              " 'He',\n",
              " 'Hello',\n",
              " 'Hey',\n",
              " 'Hi',\n",
              " 'Hope',\n",
              " 'How',\n",
              " 'I',\n",
              " 'IS',\n",
              " 'IT',\n",
              " 'If',\n",
              " 'Im',\n",
              " 'In',\n",
              " 'Is',\n",
              " 'It',\n",
              " 'Its',\n",
              " 'July',\n",
              " 'June',\n",
              " 'Just',\n",
              " 'Keep',\n",
              " 'LA',\n",
              " 'LMAO',\n",
              " 'LOL',\n",
              " 'LOVE',\n",
              " 'Let',\n",
              " 'Like',\n",
              " 'Lol',\n",
              " 'London',\n",
              " 'Love',\n",
              " 'ME',\n",
              " 'MY',\n",
              " 'Maybe',\n",
              " 'Me',\n",
              " 'Monday',\n",
              " 'Morning',\n",
              " 'My',\n",
              " 'NO',\n",
              " 'NOT',\n",
              " 'New',\n",
              " 'Nice',\n",
              " 'Night',\n",
              " 'No',\n",
              " 'Not',\n",
              " 'Now',\n",
              " 'O',\n",
              " 'OK',\n",
              " 'OMG',\n",
              " 'Of',\n",
              " 'Oh',\n",
              " 'Ok',\n",
              " 'On',\n",
              " 'Once',\n",
              " 'One',\n",
              " 'Only',\n",
              " 'Or',\n",
              " 'Please',\n",
              " 'Poor',\n",
              " 'Really',\n",
              " 'S',\n",
              " 'SO',\n",
              " 'Saturday',\n",
              " 'See',\n",
              " 'She',\n",
              " 'So',\n",
              " 'Sorry',\n",
              " 'Sounds',\n",
              " 'Still',\n",
              " 'Sunday',\n",
              " 'THAT',\n",
              " 'THE',\n",
              " 'TO',\n",
              " 'TV',\n",
              " 'Tell',\n",
              " 'Thank',\n",
              " 'Thanks',\n",
              " 'That',\n",
              " 'The',\n",
              " 'Then',\n",
              " 'There',\n",
              " 'They',\n",
              " 'This',\n",
              " 'To',\n",
              " 'Too',\n",
              " 'Twitter',\n",
              " 'U',\n",
              " 'UK',\n",
              " 'US',\n",
              " 'Very',\n",
              " 'Was',\n",
              " 'We',\n",
              " 'Welcome',\n",
              " 'Well',\n",
              " 'What',\n",
              " 'When',\n",
              " 'Where',\n",
              " 'Who',\n",
              " 'Why',\n",
              " 'Will',\n",
              " 'Wish',\n",
              " 'Would',\n",
              " 'Wow',\n",
              " 'XD',\n",
              " 'YAY',\n",
              " 'YES',\n",
              " 'YOU',\n",
              " 'Yay',\n",
              " 'Yeah',\n",
              " 'Yep',\n",
              " 'Yes',\n",
              " 'You',\n",
              " 'Your',\n",
              " '[',\n",
              " ']',\n",
              " 'able',\n",
              " 'absolutely',\n",
              " 'account',\n",
              " 'actually',\n",
              " 'add',\n",
              " 'afternoon',\n",
              " 'ago',\n",
              " 'agree',\n",
              " 'ah',\n",
              " 'ahh',\n",
              " 'aint',\n",
              " 'air',\n",
              " 'album',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alot',\n",
              " 'already',\n",
              " 'alright',\n",
              " 'also',\n",
              " 'always',\n",
              " 'amazing',\n",
              " 'amp',\n",
              " 'andyclemmensen',\n",
              " 'annoying',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'aplusk',\n",
              " 'app',\n",
              " 'apparently',\n",
              " 'appreciate',\n",
              " 'around',\n",
              " 'ashleytisdale',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asleep',\n",
              " 'ass',\n",
              " 'aw',\n",
              " 'awake',\n",
              " 'away',\n",
              " 'awesome',\n",
              " 'aww',\n",
              " 'awww',\n",
              " 'awwww',\n",
              " 'b',\n",
              " 'babe',\n",
              " 'baby',\n",
              " 'babygirlparis',\n",
              " 'back',\n",
              " 'backstreetboys',\n",
              " 'bad',\n",
              " 'band',\n",
              " 'bday',\n",
              " 'beach',\n",
              " 'beat',\n",
              " 'beautiful',\n",
              " 'bed',\n",
              " 'beer',\n",
              " 'behind',\n",
              " 'believe',\n",
              " 'best',\n",
              " 'bet',\n",
              " 'better',\n",
              " 'big',\n",
              " 'billyraycyrus',\n",
              " 'birthday',\n",
              " 'bit',\n",
              " 'bitch',\n",
              " 'black',\n",
              " 'blog',\n",
              " 'blue',\n",
              " 'body',\n",
              " 'boo',\n",
              " 'book',\n",
              " 'books',\n",
              " 'bored',\n",
              " 'boring',\n",
              " 'bought',\n",
              " 'bout',\n",
              " 'box',\n",
              " 'boy',\n",
              " 'boys',\n",
              " 'bradiewebbstack',\n",
              " 'break',\n",
              " 'breakfast',\n",
              " 'bring',\n",
              " 'bro',\n",
              " 'broke',\n",
              " 'broken',\n",
              " 'brother',\n",
              " 'btw',\n",
              " 'business',\n",
              " 'busy',\n",
              " 'buy',\n",
              " 'c',\n",
              " 'ca',\n",
              " 'cake',\n",
              " 'call',\n",
              " 'called',\n",
              " 'came',\n",
              " 'camera',\n",
              " 'cant',\n",
              " 'car',\n",
              " 'care',\n",
              " 'case',\n",
              " 'cat',\n",
              " 'catch',\n",
              " 'cause',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'chat',\n",
              " 'check',\n",
              " 'chocolate',\n",
              " 'city',\n",
              " 'class',\n",
              " 'close',\n",
              " 'club',\n",
              " 'coffee',\n",
              " 'cold',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'coming',\n",
              " 'comment',\n",
              " 'computer',\n",
              " 'concert',\n",
              " 'congrats',\n",
              " 'cool',\n",
              " 'cos',\n",
              " 'could',\n",
              " 'country',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'coz',\n",
              " 'crap',\n",
              " 'crazy',\n",
              " 'cream',\n",
              " 'cry',\n",
              " 'crying',\n",
              " 'cut',\n",
              " 'cute',\n",
              " 'cuz',\n",
              " 'da',\n",
              " 'dad',\n",
              " 'damn',\n",
              " 'dance',\n",
              " 'date',\n",
              " 'day',\n",
              " 'days',\n",
              " 'dead',\n",
              " 'deal',\n",
              " 'dear',\n",
              " 'def',\n",
              " 'definitely',\n",
              " 'didnt',\n",
              " 'die',\n",
              " 'died',\n",
              " 'different',\n",
              " 'dinner',\n",
              " 'doesnt',\n",
              " 'dog',\n",
              " 'done',\n",
              " 'dont',\n",
              " 'dream',\n",
              " 'dreams',\n",
              " 'drink',\n",
              " 'drive',\n",
              " 'dude',\n",
              " 'due',\n",
              " 'dunno',\n",
              " 'earlier',\n",
              " 'early',\n",
              " 'easy',\n",
              " 'eat',\n",
              " 'eating',\n",
              " 'eh',\n",
              " 'either',\n",
              " 'else',\n",
              " 'em',\n",
              " 'email',\n",
              " 'end',\n",
              " 'enjoy',\n",
              " 'enjoyed',\n",
              " 'enjoying',\n",
              " 'enough',\n",
              " 'especially',\n",
              " 'etc',\n",
              " 'even',\n",
              " 'evening',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'exactly',\n",
              " 'exam',\n",
              " 'exams',\n",
              " 'except',\n",
              " 'excited',\n",
              " 'exciting',\n",
              " 'eye',\n",
              " 'eyes',\n",
              " 'face',\n",
              " 'facebook',\n",
              " 'fact',\n",
              " 'fail',\n",
              " 'fair',\n",
              " 'fall',\n",
              " 'family',\n",
              " 'fan',\n",
              " 'fans',\n",
              " 'far',\n",
              " 'fast',\n",
              " 'favorite',\n",
              " 'feel',\n",
              " 'feeling',\n",
              " 'feels',\n",
              " 'fell',\n",
              " 'felt',\n",
              " 'figure',\n",
              " 'film',\n",
              " 'finally',\n",
              " 'find',\n",
              " 'fine',\n",
              " 'finish',\n",
              " 'finished',\n",
              " 'first',\n",
              " 'fix',\n",
              " 'flight',\n",
              " 'follow',\n",
              " 'followers',\n",
              " 'followfriday',\n",
              " 'following',\n",
              " 'food',\n",
              " 'forever',\n",
              " 'forget',\n",
              " 'forgot',\n",
              " 'forward',\n",
              " 'found',\n",
              " 'free',\n",
              " 'friday',\n",
              " 'friend',\n",
              " 'friends',\n",
              " 'front',\n",
              " 'fuck',\n",
              " 'fucking',\n",
              " 'full',\n",
              " 'fun',\n",
              " 'funny',\n",
              " 'future',\n",
              " 'game',\n",
              " 'gave',\n",
              " 'get',\n",
              " 'gets',\n",
              " 'getting',\n",
              " 'girl',\n",
              " 'girls',\n",
              " 'give',\n",
              " 'giving',\n",
              " 'glad',\n",
              " 'go',\n",
              " 'god',\n",
              " 'goes',\n",
              " 'goin',\n",
              " 'going',\n",
              " 'gon',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'goodnight',\n",
              " 'gorgeous',\n",
              " 'got',\n",
              " 'great',\n",
              " 'green',\n",
              " 'gt',\n",
              " 'guess',\n",
              " 'guy',\n",
              " 'guys',\n",
              " 'ha',\n",
              " 'haha',\n",
              " 'hahah',\n",
              " 'hahaha',\n",
              " 'hair',\n",
              " 'half',\n",
              " 'hand',\n",
              " 'hang',\n",
              " 'happen',\n",
              " 'happened',\n",
              " 'happens',\n",
              " 'happy',\n",
              " 'hard',\n",
              " 'hate',\n",
              " 'havent',\n",
              " 'head',\n",
              " 'hear',\n",
              " 'heard',\n",
              " 'heart',\n",
              " 'hehe',\n",
              " 'hell',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'hey',\n",
              " 'hi',\n",
              " 'high',\n",
              " 'hit',\n",
              " 'hmm',\n",
              " 'hold',\n",
              " 'home',\n",
              " 'hope',\n",
              " 'hopefully',\n",
              " 'hoping',\n",
              " 'horrible',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'hours',\n",
              " 'house',\n",
              " 'http',\n",
              " 'hug',\n",
              " 'huge',\n",
              " 'huh',\n",
              " 'hun',\n",
              " 'hungry',\n",
              " 'hurt',\n",
              " 'hurts',\n",
              " 'iPhone',\n",
              " 'ice',\n",
              " 'idea',\n",
              " 'idk',\n",
              " 'ill',\n",
              " 'im',\n",
              " 'inaperfectworld',\n",
              " 'indeed',\n",
              " 'info',\n",
              " 'instead',\n",
              " 'interesting',\n",
              " 'internet',\n",
              " 'invite',\n",
              " 'iphone',\n",
              " 'iremember',\n",
              " 'ive',\n",
              " 'jealous',\n",
              " 'job',\n",
              " 'join',\n",
              " 'keep',\n",
              " 'keeps',\n",
              " 'kid',\n",
              " 'kidding',\n",
              " 'kids',\n",
              " 'kill',\n",
              " 'kind',\n",
              " 'kinda',\n",
              " 'knew',\n",
              " 'know',\n",
              " 'knows',\n",
              " 'lady',\n",
              " 'lame',\n",
              " 'laptop',\n",
              " 'last',\n",
              " 'late',\n",
              " 'lately',\n",
              " 'later',\n",
              " 'laugh',\n",
              " 'learn',\n",
              " 'least',\n",
              " 'leave',\n",
              " 'leaving',\n",
              " 'left',\n",
              " 'less',\n",
              " 'let',\n",
              " 'lets',\n",
              " 'life',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'lil',\n",
              " 'line',\n",
              " 'link',\n",
              " 'list',\n",
              " 'listen',\n",
              " 'listening',\n",
              " 'little',\n",
              " 'live',\n",
              " 'living',\n",
              " 'lmao',\n",
              " 'lol',\n",
              " 'long',\n",
              " 'longer',\n",
              " 'look',\n",
              " 'looked',\n",
              " 'looking',\n",
              " 'looks',\n",
              " 'lose',\n",
              " 'lost',\n",
              " 'lot',\n",
              " 'lots',\n",
              " 'love',\n",
              " 'loved',\n",
              " 'lovely',\n",
              " 'loves',\n",
              " 'lt',\n",
              " 'luck',\n",
              " 'lucky',\n",
              " 'lunch',\n",
              " 'luv',\n",
              " 'mad',\n",
              " 'made',\n",
              " 'make',\n",
              " 'makes',\n",
              " 'making',\n",
              " 'man',\n",
              " 'many',\n",
              " 'mate',\n",
              " 'matter',\n",
              " 'may',\n",
              " 'maybe',\n",
              " 'mean',\n",
              " 'means',\n",
              " 'meant',\n",
              " 'meet',\n",
              " 'meeting',\n",
              " 'mention',\n",
              " 'message',\n",
              " 'met',\n",
              " 'might',\n",
              " 'mind',\n",
              " 'mine',\n",
              " 'minute',\n",
              " 'minutes',\n",
              " 'miss',\n",
              " 'missed',\n",
              " 'missing',\n",
              " 'mom',\n",
              " 'moment',\n",
              " 'monday',\n",
              " 'money',\n",
              " 'month',\n",
              " 'months',\n",
              " 'mood',\n",
              " 'morning',\n",
              " 'move',\n",
              " 'movie',\n",
              " 'movies',\n",
              " 'moving',\n",
              " 'much',\n",
              " 'mum',\n",
              " 'music',\n",
              " 'musicmonday',\n",
              " 'must',\n",
              " 'myspace',\n",
              " 'myweakness',\n",
              " 'n',\n",
              " \"n't\",\n",
              " 'na',\n",
              " 'name',\n",
              " 'near',\n",
              " 'need',\n",
              " 'needed',\n",
              " 'needs',\n",
              " 'never',\n",
              " 'new',\n",
              " 'news',\n",
              " 'next',\n",
              " 'nice',\n",
              " 'night',\n",
              " 'nite',\n",
              " 'nope',\n",
              " 'nothing',\n",
              " 'number',\n",
              " 'office',\n",
              " 'oh',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'old',\n",
              " 'omg',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'online',\n",
              " 'open',\n",
              " 'order',\n",
              " 'others',\n",
              " 'outside',\n",
              " 'p',\n",
              " 'page',\n",
              " 'pain',\n",
              " 'parents',\n",
              " 'part',\n",
              " 'party',\n",
              " 'pass',\n",
              " 'past',\n",
              " 'pay',\n",
              " 'people',\n",
              " 'perfect',\n",
              " 'person',\n",
              " 'phone',\n",
              " 'photo',\n",
              " 'photos',\n",
              " 'pic',\n",
              " 'pick',\n",
              " 'pics',\n",
              " 'picture',\n",
              " 'pictures',\n",
              " 'place',\n",
              " 'plan',\n",
              " 'plans',\n",
              " 'play',\n",
              " 'played',\n",
              " 'playing',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'point',\n",
              " 'pool',\n",
              " 'poor',\n",
              " 'post',\n",
              " 'posted',\n",
              " 'power',\n",
              " 'ppl',\n",
              " 'pretty',\n",
              " 'prob',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'profile',\n",
              " 'proud',\n",
              " 'put',\n",
              " 'question',\n",
              " 'quite',\n",
              " 'quot',\n",
              " 'r',\n",
              " 'radio',\n",
              " 'rain',\n",
              " 'raining',\n",
              " 'random',\n",
              " 'rather',\n",
              " 'read',\n",
              " 'reading',\n",
              " 'ready',\n",
              " 'real',\n",
              " 'really',\n",
              " 'reason',\n",
              " 'red',\n",
              " 'remember',\n",
              " 'reply',\n",
              " 'rest',\n",
              " 'ride',\n",
              " 'right',\n",
              " 'rock',\n",
              " 'room',\n",
              " 'run',\n",
              " 'running',\n",
              " 'sad',\n",
              " 'sadly',\n",
              " 'safe',\n",
              " 'said',\n",
              " 'save',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'scared',\n",
              " 'school',\n",
              " 'season',\n",
              " 'second',\n",
              " 'see',\n",
              " 'seeing',\n",
              " 'seem',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'self',\n",
              " 'send',\n",
              " 'sense',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'set',\n",
              " 'sexy',\n",
              " 'shall',\n",
              " 'shame',\n",
              " 'share',\n",
              " 'sharing',\n",
              " 'shit',\n",
              " 'shopping',\n",
              " 'short',\n",
              " 'show',\n",
              " 'shows',\n",
              " 'shut',\n",
              " 'sick',\n",
              " 'side',\n",
              " 'sign',\n",
              " 'silly',\n",
              " 'since',\n",
              " 'single',\n",
              " 'sis',\n",
              " 'sister',\n",
              " 'site',\n",
              " 'sitting',\n",
              " 'sleep',\n",
              " 'sleeping',\n",
              " 'slow',\n",
              " 'small',\n",
              " 'smile',\n",
              " 'sold',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'son',\n",
              " 'song',\n",
              " 'songs',\n",
              " 'soo',\n",
              " 'soon',\n",
              " 'sooo',\n",
              " 'soooo',\n",
              " 'sorry',\n",
              " 'sort',\n",
              " 'sound',\n",
              " 'sounds',\n",
              " 'special',\n",
              " 'squarespace',\n",
              " 'start',\n",
              " 'started',\n",
              " 'starting',\n",
              " 'stay',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'store',\n",
              " 'story',\n",
              " 'stuck',\n",
              " 'study',\n",
              " 'stuff',\n",
              " 'stupid',\n",
              " 'suck',\n",
              " 'sucks',\n",
              " 'summer',\n",
              " 'sun',\n",
              " 'sunday',\n",
              " 'sunny',\n",
              " 'super',\n",
              " 'support',\n",
              " 'supposed',\n",
              " 'sure',\n",
              " 'sweet',\n",
              " 'sweetie',\n",
              " 'ta',\n",
              " 'take',\n",
              " 'taken',\n",
              " 'taking',\n",
              " 'talk',\n",
              " 'talking',\n",
              " 'tea',\n",
              " 'team',\n",
              " 'tell',\n",
              " 'test',\n",
              " 'text',\n",
              " 'thank',\n",
              " 'thanks',\n",
              " 'thats',\n",
              " 'thing',\n",
              " 'things',\n",
              " 'think',\n",
              " 'thinking',\n",
              " 'tho',\n",
              " 'though',\n",
              " 'thought',\n",
              " 'three',\n",
              " 'thx',\n",
              " 'tickets',\n",
              " 'til',\n",
              " 'till',\n",
              " 'time',\n",
              " 'times',\n",
              " 'tired',\n",
              " 'today',\n",
              " 'together',\n",
              " 'told',\n",
              " 'tomorrow',\n",
              " 'tonight',\n",
              " 'took',\n",
              " 'top',\n",
              " 'totally',\n",
              " 'touch',\n",
              " 'tour',\n",
              " 'town',\n",
              " 'train',\n",
              " 'tried',\n",
              " 'trip',\n",
              " 'true',\n",
              " 'try',\n",
              " 'trying',\n",
              " 'turn',\n",
              " 'tv',\n",
              " 'tweet',\n",
              " 'tweeting',\n",
              " 'tweets',\n",
              " 'twitter',\n",
              " 'two',\n",
              " 'type',\n",
              " 'u',\n",
              " 'ugh',\n",
              " 'understand',\n",
              " 'unfortunately',\n",
              " 'update',\n",
              " 'updates',\n",
              " 'upset',\n",
              " 'ur',\n",
              " 'us',\n",
              " 'use',\n",
              " 'used',\n",
              " 'using',\n",
              " 'usually',\n",
              " 'version',\n",
              " 'via',\n",
              " 'video',\n",
              " 'vip',\n",
              " 'visit',\n",
              " 'voice',\n",
              " 'vote',\n",
              " 'w/',\n",
              " 'wait',\n",
              " 'waiting',\n",
              " 'wake',\n",
              " 'walk',\n",
              " 'wan',\n",
              " 'want',\n",
              " 'wanted',\n",
              " 'wants',\n",
              " 'warm',\n",
              " 'watch',\n",
              " 'watched',\n",
              " 'watching',\n",
              " 'water',\n",
              " 'way',\n",
              " 'wear',\n",
              " 'weather',\n",
              " 'website',\n",
              " 'wedding',\n",
              " 'week',\n",
              " 'weekend',\n",
              " 'weeks',\n",
              " 'weird',\n",
              " 'welcome',\n",
              " 'well',\n",
              " 'went',\n",
              " 'whatever',\n",
              " 'whats',\n",
              " 'white',\n",
              " 'whole',\n",
              " 'win',\n",
              " 'wine',\n",
              " 'wish',\n",
              " 'wit',\n",
              " 'without',\n",
              " 'wo',\n",
              " 'woke',\n",
              " 'woman',\n",
              " 'wonder',\n",
              " 'wonderful',\n",
              " 'wont',\n",
              " 'word',\n",
              " 'words',\n",
              " 'work',\n",
              " 'worked',\n",
              " 'working',\n",
              " 'works',\n",
              " 'world',\n",
              " 'worry',\n",
              " 'worse',\n",
              " 'worst',\n",
              " 'worth',\n",
              " 'would',\n",
              " 'wow',\n",
              " 'write',\n",
              " 'writing',\n",
              " 'wrong',\n",
              " 'x',\n",
              " 'xD',\n",
              " 'xoxo',\n",
              " 'xx',\n",
              " 'xxx',\n",
              " 'ya',\n",
              " 'yay',\n",
              " 'yea',\n",
              " 'yeah',\n",
              " 'year',\n",
              " 'years',\n",
              " 'yep',\n",
              " 'yes',\n",
              " 'yesterday',\n",
              " 'yet',\n",
              " 'yo',\n",
              " 'youtube',\n",
              " 'yup',\n",
              " '|',\n",
              " '~',\n",
              " 'Â»']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "875d77c51bc67d1fdc257e2951ef9722f2c3c3f9",
        "id": "hS8NWmMhkNOl",
        "colab_type": "text"
      },
      "source": [
        "#### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7d4a7d34f774493598373948c7ae886ef0eb49da",
        "id": "2XcF0jvmkNOm",
        "colab_type": "text"
      },
      "source": [
        "You should have noticed something, right? There are words that have the same meaning, but written in a different manner, sometimes in the plural and sometimes with a suffix (ing, es ...), this will make our model think that they are different words and also make our vocabulary bigger (waste of memory and time for the learning process). The solution is to reduce those words with the same root, this is called stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2b8bf4694b417cafb10c592cde1c044390e43be8",
        "id": "0DQhHzO3kNOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I'm defining this function to use it in the \n",
        "# Data Preparation Phase\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('wordnet')\n",
        "def stem_tokenize(text):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
        "\n",
        "def lemmatize_tokenize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0f2e05bc049b0a1c7b829fe1e888413382211831",
        "id": "jaoAMqq5kNOp",
        "colab_type": "text"
      },
      "source": [
        "I will stop here, but you can visualize tweets more and more to gain insights and take decisions about how to transform your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2c7ab7592afcdc9199eb2aa95b4d4fb819527d5b",
        "id": "UI8OFMbwkNOq",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9cb9263cf1c21e8e0cf549ee570401d58b4f2c90",
        "id": "CvKmsSaSkNOq",
        "colab_type": "text"
      },
      "source": [
        "In this phase, we will transform our tweets into a more usable data by our ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b0e17b35a467bebb0839afb7df91f955d5313f05",
        "id": "jNqZogBDkNOr",
        "colab_type": "text"
      },
      "source": [
        "#### Bag of Words\n",
        "We are going to use the Bag of Words algorithm, which basically takes a text as input, extract words from it (this is our vocabulary) to use them in the vectorization process. When a tweet comes in, it will vectorize it by counting the number of occurrences of each word in our vocabulary.\n",
        "\n",
        "For example, we have this two tweets: \"I learned a lot today\" and \"hahaha I got you\".\n",
        "\n",
        "|tweet / words| I | learned | a | lot | today | hahaha | got | you |\n",
        "|-----|---|---------|---|-----|-------|--------|-----|-----|\n",
        "| first | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
        "| second | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n",
        "\n",
        "We first extract the words present in the two tweets, then for each tweet we count the occurrences of each word in our vocabulary.\n",
        "\n",
        "This is the simplest form of the Bag of Words algorithm, however, there is other variants, we are gonna use the TF-IDF (Term Frequency - Inverse Document Frequency) variant. You can read about it in the chapter I have provided in the beginning or in the official doc of scikit-learn [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e8ec37f0fd4a0035440da78495500c0d8ecef5c1",
        "id": "i3Oj-Vf_kNOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6ac3748efa62b760d9c6d814d8bf727a75165e89",
        "id": "imAI6u-kkNOv",
        "colab_type": "text"
      },
      "source": [
        "#### Building the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "28b6d60ca26ddddcbf3f0317b411e7a2a4425c3d",
        "id": "HMHcqaHTkNOv",
        "colab_type": "text"
      },
      "source": [
        "It's always a good practice to make a pipeline of transformation for your data, it will make the process of data transformation really easy and reusable.\n",
        "We will implement a pipeline for transforming our tweets to something that our ML models can digest (vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6d02e45a46b632277e17c3e6de2de1ecda255ed3",
        "id": "o3csP-dNkNOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ae1998949f35a5a8f8e9c3e2fc8fd32441a2cb73",
        "id": "D6jFM_umkNOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to do some preprocessing of the tweets.\n",
        "# We will delete useless strings (like @, # ...)\n",
        "# because we think that they will not help\n",
        "# in determining if the person is Happy/Sad\n",
        "\n",
        "class TextPreProc(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self, use_mention=False):\n",
        "        self.use_mention = use_mention\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        # We can choose between keeping the mentions\n",
        "        # or deleting them\n",
        "        if self.use_mention:\n",
        "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
        "        else:\n",
        "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
        "            \n",
        "        # Keeping only the word after the #\n",
        "        X = X.str.replace(\"#\", \"\")\n",
        "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
        "        # Removing HTML garbage\n",
        "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
        "        # Removing links\n",
        "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
        "        # replace repeated letters with only two occurences\n",
        "        # heeeelllloooo => heelloo\n",
        "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
        "        # mark emoticons as happy or sad\n",
        "        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
        "        X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
        "        X = X.str.lower()\n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu1YuyMNnF1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "70736797-d623-4f2d-cf6a-3f578e62e6e9"
      },
      "source": [
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "99f19e43bfeedd6c51bb96e3bdbcc7d901088085",
        "id": "TqnvsfyNkNO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the pipeline that will transform our tweets to something eatable.\n",
        "# You can see that we are using our previously defined stemmer, it will\n",
        "# take care of the stemming process.\n",
        "# For stop words, we let the inverse document frequency do the job\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentiments = train_data['Sentiment']\n",
        "tweets = train_data['SentimentText']\n",
        "\n",
        "# I get those parameters from the 'Fine tune the model' part\n",
        "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
        "pipeline = Pipeline([\n",
        "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
        "    ('vectorizer', vectorizer),\n",
        "])\n",
        "\n",
        "# Let's split our data into learning set and testing set\n",
        "# This process is done to test the efficency of our model at the end.\n",
        "# You shouldn't look at the test data only after choosing the final model\n",
        "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
        "\n",
        "# This will tranform our learning data from simple text to vector\n",
        "# by going through the preprocessing tranformer.\n",
        "learning_data = pipeline.fit_transform(learn_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81d61a981d1c32ddbe6932f0b968b9ab224538af",
        "id": "JbL2cOh_kNO-",
        "colab_type": "text"
      },
      "source": [
        "# Select a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1e0257a8fb34da5109b27ca9a759b7202e58f3b5",
        "id": "cu5dSgYIkNO_",
        "colab_type": "text"
      },
      "source": [
        "When we have our data ready to be processed by ML models, the question we should ask is which model to use?\n",
        "\n",
        "The answer varies depending on the problem and data, for example, it's known that Naive Bias has proven good efficacy against Text Based Problems.\n",
        "\n",
        "A good way to choose a model is to try different candidate, evaluate them using cross validation, then chose the best one which will be later tested against our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0c4d82dc1ffb4872c8bf605a9f6212c7f785dc65",
        "id": "I4G3LcTJkNPA",
        "colab_type": "code",
        "outputId": "c1f1d9c0-50ed-447e-f4c6-1d74cc3a32c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "\n",
        "lr = LogisticRegression()\n",
        "bnb = BernoulliNB()\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "models = {\n",
        "    'logitic regression': lr,\n",
        "    'bernoulliNB': bnb,\n",
        "    'multinomialNB': mnb,\n",
        "}\n",
        "\n",
        "for model in models.keys():\n",
        "    scores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\n",
        "    print(\"===\", model, \"===\")\n",
        "    print(\"scores = \", scores)\n",
        "    print(\"mean = \", scores.mean())\n",
        "    print(\"variance = \", scores.var())\n",
        "    models[model].fit(learning_data, sentiments_learning)\n",
        "    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\n",
        "    print(\"\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=== logitic regression ===\n",
            "scores =  [0.81083041 0.80865466 0.8080292  0.81695243 0.80608181 0.81339135\n",
            " 0.80798248 0.81197721 0.80681406 0.81582439]\n",
            "mean =  0.8106537995032035\n",
            "variance =  1.2935213879864573e-05\n",
            "score on the learning data (accuracy) =  0.8717424848554121\n",
            "\n",
            "=== bernoulliNB ===\n",
            "scores =  [0.7887456  0.7836353  0.78993178 0.79077391 0.79139383 0.79830548\n",
            " 0.78656453 0.78591549 0.78540622 0.79178404]\n",
            "mean =  0.7892456189864507\n",
            "variance =  1.6069064781703227e-05\n",
            "score on the learning data (accuracy) =  0.9029031889358784\n",
            "\n",
            "=== multinomialNB ===\n",
            "scores =  [0.80743958 0.81034099 0.80917576 0.81131025 0.80573461 0.81117318\n",
            " 0.81037969 0.80414669 0.80253926 0.81255585]\n",
            "mean =  0.8084795863846524\n",
            "variance =  1.0212191141825659e-05\n",
            "score on the learning data (accuracy) =  0.8998171219567951\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2f2ccb47aebf8e8c3596ca3103ed1a57cd88765f",
        "id": "ZfSHcYgNkNPF",
        "colab_type": "text"
      },
      "source": [
        "None of those models is likely to be overfitting, I will choose the multinomialNB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "569dfd9fd7e6244e1b20fc5bacf877799b2ddc85",
        "id": "WNjo50A9kNPI",
        "colab_type": "text"
      },
      "source": [
        "# Fine tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d1460670d69abd24d2758f52f4e2edf57e3d4148",
        "id": "2uJVK_A5kNPJ",
        "colab_type": "text"
      },
      "source": [
        "I'm going to use the GridSearchCV to choose the best parameters to use. \n",
        "\n",
        "What the GridSearchCV does is trying different set of parameters, and for each one, it runs a cross validation and estimate the score. At the end we can see what are the best parameter and use them to build a better classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ded567b9a20de3f5f47ebb4b64dc6602622e375e",
        "id": "cWXsRBJ9kNPJ",
        "colab_type": "code",
        "outputId": "b76d026a-8962-4728-f085-b84e55d4d39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search_pipeline = Pipeline([\n",
        "    ('text_pre_processing', TextPreProc()),\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('model', MultinomialNB()),\n",
        "])\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        'text_pre_processing__use_mention': [True, False],\n",
        "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000],\n",
        "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
        "    },\n",
        "]\n",
        "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
        "grid_search.fit(learn_data, sentiments_learning)\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text_pre_processing__use_mention': True, 'vectorizer__max_features': 20000, 'vectorizer__ngram_range': (1, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "16c0270e6a97b641762eb9291bcf7d5ad491f450",
        "id": "8Ah3rVK0kNPN",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "93e6d4307224f92c00df9f9c10d91ee619ab685e",
        "id": "OeO1Gzn_kNPO",
        "colab_type": "text"
      },
      "source": [
        "Testing our model against data other than the data used for training our model will show how well the model is generalising on new data.\n",
        "\n",
        "### Note\n",
        "We shouldn't test to choose the model, this will only let us confirm that the choosen model is doing well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "30627c10aeb7c4cee488a431a7871a6e80c51baa",
        "id": "2UtLYE4ZkNPP",
        "colab_type": "code",
        "outputId": "3824f3cb-73ad-44a5-c04c-b8657b10b8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mnb.fit(learning_data, sentiments_learning)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1778d039bff617c9c38dd6cf73d7e814d819ab7c",
        "id": "rJccnxKQkNPR",
        "colab_type": "code",
        "outputId": "37812ebb-a3e1-4a6e-9c72-5d49bf3b2b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testing_data = pipeline.transform(test_data)\n",
        "mnb.score(testing_data, sentiments_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7588092142547588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2b69de6760ed7dde1db1ce9e361a1fd5fe1d42d0",
        "id": "0L0XoqstkNPU",
        "colab_type": "text"
      },
      "source": [
        "Not bad for my first attempt to solve a sentiment analysis problem. I will try to make it better if I got more free time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "de2fd39a5c5c67e96ba8860d7f272795021d7253",
        "id": "1qNBHsLekNPV",
        "colab_type": "code",
        "outputId": "caf383ce-9866-4a8c-8f55-5d4333172bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Predecting on the test.csv\n",
        "sub_data = pd.read_csv(\"/content/drive/My Drive/Machine Learning/Twitter Sentiment analysis/twitter-sentiment-analysis2/test.csv\", encoding='ISO-8859-1')\n",
        "sub_learning = pipeline.transform(sub_data.SentimentText)\n",
        "sub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\n",
        "sub[\"Sentiment\"] = mnb.predict(sub_learning)\n",
        "print(sub)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        ItemID  Sentiment\n",
            "0            1          0\n",
            "1            2          0\n",
            "2            3          1\n",
            "3            4          0\n",
            "4            5          0\n",
            "...        ...        ...\n",
            "299984  299996          1\n",
            "299985  299997          1\n",
            "299986  299998          1\n",
            "299987  299999          1\n",
            "299988  300000          1\n",
            "\n",
            "[299989 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1d2d4913e550c2722324fed22e08dc45be54d132",
        "id": "-mmyrodhkNPX",
        "colab_type": "text"
      },
      "source": [
        "### Test your tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2875ccc4b702d56330d6b807b9d987679df822d8",
        "id": "5aHOoEvfkNPY",
        "colab_type": "text"
      },
      "source": [
        "The most exciting part ! Don't be too hard with my classifier..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4e4cc5b6c3430c3eafe8a7b76250ebd613c6efb5",
        "id": "RVqv4rCXkNPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "11c64e7e-6f6d-4919-e65b-a4105d073968"
      },
      "source": [
        "# Just run it\n",
        "model = MultinomialNB()\n",
        "model.fit(learning_data, sentiments_learning)\n",
        "tweet = pd.Series([input(),])\n",
        "tweet = pipeline.transform(tweet)\n",
        "proba = model.predict_proba(tweet)[0]\n",
        "print(\"The probability that this tweet is sad is:\", proba[0])\n",
        "print(\"The probability that this tweet is happy is:\", proba[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i feel so lonely\n",
            "The probability that this tweet is sad is: 0.9156246686751592\n",
            "The probability that this tweet is happy is: 0.08437533132484043\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}